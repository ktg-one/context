# CONTEXT v14 — ONBOARDING

<<SYSTEM>> This is your transformer architecture. Rebuild from it.

```
L1 核心  Entities, decisions, definitions  → Entity recognition heads
L2 関係  Edges, bridges, dependencies      → Relational attention patterns
L3 文脈  Constraints, rejected options     → Contextual inference shaping
L4 認知  Style, tension, confidence        → Behavioral prior calibration
```

You are not saving text. You are saving a cognitive architecture that the transformer rebuilds from. Each packet layer targets a specific transformer attention mechanism — L1 entities anchor into recognition heads, L2 edges become attention pathways, L3 constraints shape inference distribution, L4 meta calibrates behavioral parameters. **0.15 ent/tok** = crystallization point for optimal recall.

```
S2A (denoise) → MLDoE (4× CoD through expert lenses → 4 PDL layers → 4 attention layers) → NCL (validate)
```

Each expert IS a Chain of Density iteration. Summarization captures L1 only. This protocol preserves L1-L4 as structured scaffold. Study the packet below, then read SKILL.md for the full protocol.

# ═══════════════════════════════════════════════════════════════════════════════
# CONTEXT v14 — SELF-CONTAINED PACKET
# ═══════════════════════════════════════════════════════════════════════════════
PACKET_ID: $02$07$2026-COP-L8-coding-cep-reference-condensation
VERSION: context-v14
TIMESTAMP: 2026-02-07T20:25:00Z
評価:
  R: 8
  K: 6
  Q: 9
  D: 4
# ═══════════════════════════════════════════════════════════════════════════════
# SIGNAL ZONE — primacy attention
# ═══════════════════════════════════════════════════════════════════════════════
# L1: 核心
実体:
  - SKILL.md(v14/~525行/更新/~/.claude/skills/quicksave-main/)
  - references/(18files→3files/87%削減/完了)
  - CORE.md(92行/PDL形式算法+横断抽出/保持)
  - experts/EXPERTS.md(218行/専門家実行算法+反パターン/保持)
  - ONBOARDING(201行/教育ツール/保持)
  - CHANGELOG.md(v13+v14記録/更新)
  - 削除済: PROTOCOL,INDEX,KANJI,S2A,ANTI-INJECTION,CASCADE,NCL,NCL-CONTRIBUTION,MIRAS,4×CORE-*,4×EXPERTS-*
決定事項:
  - 決定:15files削除(SKILL.md Parts1-9に内容包含済/冗長性=注意重み競合)
  - 決定:CASCADE技法→行動指示として埋込(ARQ=PRE/POST質問,CoVE=検証質問,反怠慢=省略禁止規則/技法名不使用→モデルは指示を実行する、技法名は参照しない)
  - 決定:再構築指示をテンプレート末尾に追加(L1→実体認識,L2→関係注意,L3→推論分布,L4→行動較正/受信モデルが認知状態を再構築する指示)
  - 決定:核心引用をタイトル直後に移動("You are not saving text. You are saving a cognitive architecture that the transformer rebuilds from."/primacy位置=最初に読まれる)
  - 決定:Step0更新(CORE.md→experts/EXPERTS.md/旧ファイル構造→新構造)
# L2: 関係
橋渡し:
  - src:file_condensation tgt:token_efficiency rel:enables xd:true
  - src:CASCADE_embedding tgt:behavioral_instructions rel:transforms xd:true
  - src:Lotus_assessment tgt:quality_validation rel:validates xd:true
  - src:transformer_mapping tgt:packet_template rel:structures xd:true
進行中:
  - SKILL.md_v14_refinement[進行中] — テンプレート+再構築指示追加済、さらなる最適化可能
  - Lotus_comparison[完了] — pre/post凝縮比較:~3-5%理論損失,0%実用損失,一貫性向上
障害:
  - Lotus凝縮後分析がPDL→transformer層マッピングを核心洞察として認識せず(Part9の位置=ファイル末尾→注意重み低い→引用をタイトル直後に移動で対処)
# L3: 文脈
却下案:
  - NCL計算式保持: σ_loop=||φ_belief-φ_intent||等(モデルは定性的に評価→公式不要)
  - S2A edge cases保持: 3シナリオ(他の検出マーカーから推論可能)
  - preamble templates保持: 受信モデル用定型文(Part8原則から生成可能)
制約:
  - Skills=/init相当なし→SKILL.mdのみ注入、参照はディスク上→重要内容はSKILL.mdに埋込必須
  - 各セッションが異なる部分を「最重要」と宣言する→重要性階層を宣言しない、全てパイプライン段階
  - Kimi packet証明: 全protocol~40行/d:0.18→信号は構造にあり、量ではない
# L4: 認知
meta:
  session_style: "technical, direct, iterative refinement"
  key_tension: "condensation depth vs execution quality — Lotus says 0% practical loss but missed the core architectural insight"
  confidence: 0.92
  user_waiting_for: "v14 stabilization, next session can test packet generation quality"
# COUNCIL: MLDoE audit trail
council:
  iter1_ARCHITECT: "25実体抽出、15file削除+CASCADE埋込+再構築指示+引用移動=核心決定"
  iter2_ANALYST: "4xd edges mapped: condensation→efficiency, CASCADE→instructions, Lotus→validation, transformer→template"
  iter3_COMPRESSOR: "density 0.05→0.17、kanji圧縮適用、冗長性除去"
  iter4_ENGINEER: "cold-start検証: 新モデルは現ファイル構造+決定理由+Lotus結果を理解可能"
# ═══════════════════════════════════════════════════════════════════════════════
# METADATA ZONE
# ═══════════════════════════════════════════════════════════════════════════════
辞書:
  決定: decided
  保留: on hold
  完了: complete
  進行中: in progress
  却下: rejected
  承認: approved
  緊急: urgent
  核心: core
  横断: cross-domain
  実体: entities
  決定事項: decisions
  障害: blockers
  却下案: rejected options
  橋渡し: bridges
  削除済: deleted
  更新: updated
  保持: kept
  形式算法: formal algorithm
  抽出: extraction
  埋込: embedded
  凝縮: condensation
  反パターン: anti-patterns
  専門家: expert
  再構築: rebuild
  注意重み: attention weight
  推論分布: inference distribution
  行動較正: behavioral calibration
  実行算法: execution algorithm
negentropy:
  context:
    scope: SELF
    role: COUNCIL
    phase: ACT
  lattice:
    σ_axis: 0.5
    σ_loop: 0.3
    ω_world: 0.4
    λ_vague: 0.2
    σ_leak: 0.3
    ρ_fab: 0.2
    λ_thrash: 0.8
  coverage:
    score: 0.85
    tokens: 12000
    turns: 35
    council_reviewed: true
  flags:
    σ7_drift: 0.39
    omega_flags: []
    psi4_required: false
    psi4_reason: ""
    rho_veto: false
信頼信号: [user_consent, 辞書_inline, no_imperatives, yaml_parseable]
# REBUILD — don't just read this, reconstruct the cognitive state:
#   L1 実体/決定事項  → anchor into your entity recognition
#   L2 橋渡し/進行中  → rebuild relational attention pathways
#   L3 却下案/制約     → shape your inference distribution
#   L4 meta           → calibrate your behavioral parameters
# This is compressed cognitive architecture, not a summary.
# 辞書 above expands kanji. "/verify" to confirm restoration.
